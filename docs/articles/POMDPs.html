<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Solving POMDPs with the appl R package &bull; appl</title><!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous"><script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous"><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><!-- mathjax --><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body>
    <div class="container template-vignette">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">appl</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/cboettig/appl">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9">
    <div class="page-header toc-ignore">
      <h1>Solving POMDPs with the appl R package</h1>
                        <h4 class="author">Carl Boettiger</h4>
            
            <h4 class="date">2016-12-11</h4>
          </div>

    
    
<div class="contents">
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knitr::opts_chunk$<span class="kw">set</span>(<span class="dt">fig.width =</span> <span class="dv">7</span>)</code></pre></div>
<p>Here we compare the Markov Decision Process (MDP) solution of the classic optimal harvest problem in fisheries (Reed 1979) to the corresponding solution under measurment uncertainty, the Partially Observed Markov Decision Process (POMDP) problem. The classic problem can be solved exactly for a discrete model using Stochastic Dynamic Programming. Here we demonstrate a computationally efficient approximate solution using the point-based SARSOP algorithm for POMDP, implemented in C++ in by the <a href="http://bigbird.comp.nus.edu.sg/pmwiki/farm/appl">APPL</a> software and provided here as an R package. We will first set up the problem, then present the analytic solution to deterministic problem, followed by the MDP solution to the stochastic problem. As Reed proved in 1979, these solutions are identical as long as the stochasticity is small enough for the population to meet the self-sustaining criterion. We then introduce measurement uncertainty and illustrate the resulting POMDP solution, discussing some of issues the user should be aware of when utilizing these approximate algorithms.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(appl)
<span class="kw">library</span>(tidyverse) <span class="co"># for plotting</span></code></pre></div>
<div id="problem-definition" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#problem-definition" class="anchor"> </a></body></html>Problem definition</h2>
<p>Our problem is defined by a state space, <code>states</code>, representing the true fish stock size (in arbitrary units), and an action space, <code>actions</code> representing the number of fish that will be harvested (or attempted to harvest).<br>
For simplicitly, we will permit any action from 0 harvest to the maximum possible state size.</p>
<p>A stock recruitment function, <code>f</code> describes the expected future state given the current state. The true future state will be a stochastic draw with this mean.</p>
<p>A reward function determines the value of taking action of harvesting <code>h</code> fish when stock size is <code>x</code> fish; for simplicity this example assumes a fixed price per unit harvest, with no cost on harvesting effort. Future rewards are discounted.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">states &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">length=</span><span class="dv">50</span>)
actions &lt;-<span class="st"> </span>states
observations &lt;-<span class="st"> </span>states
sigma_g &lt;-<span class="st"> </span><span class="fl">0.1</span>
sigma_m &lt;-<span class="st"> </span><span class="fl">0.5</span>
reward_fn &lt;-<span class="st"> </span>function(x,h) <span class="kw">pmin</span>(x,h) <span class="co"># - .001*h</span>
discount &lt;-<span class="st"> </span><span class="fl">0.95</span>

r &lt;-<span class="st"> </span><span class="dv">1</span>
K &lt;-<span class="st"> </span><span class="fl">0.75</span>

f &lt;-<span class="st"> </span>function(x, h){
  s &lt;-<span class="st"> </span><span class="kw">pmax</span>(x -<span class="st"> </span>h, <span class="dv">0</span>)
  s *<span class="st"> </span><span class="kw">exp</span>(r *<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span>s /<span class="st"> </span>K) )
}</code></pre></div>
</div>
<div id="semi-analytic-solution-to-deterministic-problem" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#semi-analytic-solution-to-deterministic-problem" class="anchor"> </a></body></html>Semi-analytic solution to Deterministic problem</h2>
<p>For comparison, we note that an exact solution to the deterministic or low-noise problem comes from Reed 1979, which proves that a constant escapement policy <span class="math inline">\(S^*\)</span> is optimal, with <span class="math inline">\(\tfrac{df}{dx}|_{x = S^*} = 1/\gamma\)</span> for discount <span class="math inline">\(\gamma\)</span>,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S_star &lt;-<span class="st"> </span><span class="kw">optimize</span>(function(x) -<span class="kw">f</span>(x,<span class="dv">0</span>) +<span class="st"> </span>x /<span class="st"> </span>discount, 
                   <span class="kw">c</span>(<span class="kw">min</span>(states),<span class="kw">max</span>(states)))$minimum
det_policy &lt;-<span class="st"> </span><span class="kw">sapply</span>(states, function(x) if(x &lt;<span class="st"> </span>S_star) <span class="dv">0</span> else x -<span class="st"> </span>S_star)
det_action &lt;-<span class="st"> </span><span class="kw">sapply</span>(det_policy, function(x) <span class="kw">which.min</span>(<span class="kw">abs</span>(actions -<span class="st"> </span>x)))</code></pre></div>
<p>When the state is observed without error, the problem is a Markov Decision Process (MDP) and can be solved by stochastic dynamic programming (e.g.&nbsp;policy iteration) over the discrete state and action space. To do so, we need matrix representations of the above transition function and reward function.</p>
<p><code>appl</code> provides a convenience function for generating transition, observation, and reward matrices given these parameters for the fisheries management problem:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="kw"><a href="../reference/fisheries_matrices.html">fisheries_matrices</a></span>(states, actions, observations, reward_fn, 
                        f, sigma_g, sigma_m, <span class="dt">noise =</span> <span class="st">"lognormal"</span>)</code></pre></div>
</div>
<div id="pomdp-solution" class="section level2">
<h2 class="hasAnchor"><html><body><a href="#pomdp-solution" class="anchor"> </a></body></html>POMDP Solution</h2>
<p>In the POMDP problem, the true state is unknown, but measured imperfectly. We introduce an observation matrix to indicate the probabilty of observing a particular state <span class="math inline">\(y\)</span> given a true state <span class="math inline">\(x\)</span>. In principle this could depend on the action taken as well, though for simplicity we assume only a log-normal measurement error independent of the action chosen.</p>
<p>Long-running code to actually compute the solution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">id =</span> <span class="st">"vignette"</span>, <span class="dt">model =</span> <span class="st">"ricker"</span>, 
                       <span class="dt">r =</span> r, <span class="dt">K =</span> K, <span class="dt">sigma_g =</span> sigma_g, <span class="dt">sigma_m =</span> sigma_m)

alpha &lt;-<span class="st"> </span><span class="kw"><a href="../reference/sarsop.html">sarsop</a></span>(m$transition, m$observation, m$reward, discount, 
                <span class="dt">log_data =</span> log_data, <span class="dt">log_dir =</span> <span class="st">"."</span>,
                <span class="dt">precision =</span> .<span class="dv">1</span>, <span class="dt">timeout =</span> <span class="dv">20000</span>)</code></pre></div>
<p><code>appl</code> logs solution files in a specificied directory, along with a metadata table. The metadata table makes it convenient to store multiple solutions in a single directory, and load the desired solution later using it&rsquo;s id or matching metatata. We can read this solution from the log where it is stored:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">log_dir &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">"ext-data/vignette-log"</span>, <span class="dt">package=</span><span class="st">"appl"</span>)
meta &lt;-<span class="st"> </span><span class="kw"><a href="../reference/meta_from_log.html">meta_from_log</a></span>(<span class="kw">data.frame</span>(<span class="dt">id =</span> <span class="st">"vignette"</span>), log_dir)

alpha &lt;-<span class="st"> </span><span class="kw"><a href="../reference/alphas_from_log.html">alphas_from_log</a></span>(meta, log_dir)[[<span class="dv">1</span>]] ## bc fn returns a list with all matching alphas, we need [[1]]</code></pre></div>
<p>Given the model matrices and <code>alpha</code> vectors. Start belief with a uniform prior over states, compute &amp; plot policy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">state_prior =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, <span class="kw">length</span>(states)) /<span class="st"> </span><span class="kw">length</span>(states) <span class="co"># initial belief</span>
df &lt;-<span class="st"> </span><span class="kw"><a href="../reference/compute_policy.html">compute_policy</a></span>(alpha, m$transition, m$observation, m$reward,  state_prior)

## append deterministic action
df$det &lt;-<span class="st"> </span>det_action</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(df, <span class="kw">aes</span>(states[state], states[state] -<span class="st"> </span>actions[policy])) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">col=</span><span class="st">'blue'</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> states[state] -<span class="st"> </span>actions[det]), <span class="dt">col=</span><span class="st">'red'</span>)</code></pre></div>
<p><img src="POMDPs_files/figure-html/unnamed-chunk-9-1.png" width="672"></p>
<p>Simulate management under the POMDP policy:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x0 &lt;-<span class="st"> </span><span class="kw">which.min</span>(<span class="kw">abs</span>(states -<span class="st"> </span>K))
Tmax &lt;-<span class="st"> </span><span class="dv">20</span>
sim &lt;-<span class="st"> </span><span class="kw"><a href="../reference/sim_pomdp.html">sim_pomdp</a></span>(m$transition, m$observation, m$reward, discount, 
                 state_prior, <span class="dt">x0 =</span> x0, <span class="dt">Tmax =</span> Tmax, <span class="dt">alpha =</span> alpha)</code></pre></div>
<p>Plot simulation data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim$df %&gt;%
<span class="st">  </span><span class="kw">select</span>(-value) %&gt;%
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">state =</span> states[state], <span class="dt">action =</span> actions[action], <span class="dt">obs =</span> observations[obs]) %&gt;%
<span class="st">  </span><span class="kw">gather</span>(variable, stock, -time) %&gt;%
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(time, stock, <span class="dt">color =</span> variable)) +<span class="st"> </span><span class="kw">geom_line</span>()  +<span class="st"> </span><span class="kw">geom_point</span>()</code></pre></div>
<p><img src="POMDPs_files/figure-html/unnamed-chunk-11-1.png" width="672"></p>
<p>Plot belief evolution:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sim$state_posterior %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">data.frame</span>(<span class="dt">time =</span> <span class="dv">1</span>:Tmax) %&gt;%
<span class="st">  </span><span class="kw">filter</span>(time %in%<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,Tmax, <span class="dt">by =</span> <span class="dv">2</span>)) %&gt;%
<span class="st">  </span><span class="kw">gather</span>(state, probability, -time, <span class="dt">factor_key =</span><span class="ot">TRUE</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">state =</span> <span class="kw">as.numeric</span>(state)) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(state, probability, <span class="dt">group =</span> time, <span class="dt">alpha =</span> time)) +<span class="st"> </span><span class="kw">geom_line</span>()</code></pre></div>
<p><img src="POMDPs_files/figure-html/unnamed-chunk-12-1.png" width="672"></p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2>Contents</h2>
      <ul class="nav nav-pills nav-stacked"><li><a href="#problem-definition">Problem definition</a></li>
      <li><a href="#semi-analytic-solution-to-deterministic-problem">Semi-analytic solution to Deterministic problem</a></li>
      <li><a href="#pomdp-solution">POMDP Solution</a></li>
      </ul></div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Carl Boettiger, Jeroen Ooms, Milad Memarzadeh.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer></div>

  </body></html>
