---
output:
  html_document: 
    keep_md: yes
    variant: markdown_github
---  



```{r}
library("MDPtoolbox")
library("appl")
library("tidyr")
library("ggplot2")
library("purrr")
knitr::opts_chunk$set(cache = TRUE)
```

```{r}
devtools::load_all()
```

Typical model setup:

```{r}

n_s <- 40
precision <- 10

states <- 0:(n_s-1)
actions <- states
obs <- states

f <- function(x, h, r = 1, K = 35){
  s <- pmax(x - h, 0)
  s * exp(r * (1 - s / K) )
}

sigma_g <- 0.2  
sigma_m <- sigma_g

reward_fn <- function(x,h) pmin(x,h)
discount <- 0.95

m <- fisheries_matrices(states, actions, obs, reward_fn, f, sigma_g, sigma_m) 
```




SARSOP computes only from attainable states, thus if the initial belief excludes most states, the calculation is much faster.
Here we compare the alpha vectors calculated from an initial belief reflecting an observation 



```{r}
system.time(unif <- compute_alpha_vectors(m$transition, m$observation, m$reward, discount, rep(1, n_s) / n_s, precision = precision))
```


```{r}
belief <- m$observation[,n_s-4,1]
system.time(K <- compute_alpha_vectors(m$transition, m$observation, m$reward, discount, belief, precision = precision))
```


```{r}
system.time(notunif <- compute_alpha_vectors(m$transition, m$observation, m$reward, discount, 1:n_s / sum(1:n_s), precision = precision))
```


## Examining alpha vectors


```{r}

optimal_policy <- function(A, O, state){
  alpha <- unname(as.data.frame(A$alpha))
  alpha_action <- A$alpha_action
  
  V <- t(alpha) %*% O 
  value <- apply(V, 2, max)
  policy <- apply(V, 2, function(x) alpha_action[which.max(x)]) + 1 # C++ pomdpsol enumerates actions starting at 0

  data.frame(policy, value, state)  
}

```


```{r}
list(unif, K, notunif) %>% 
  purrr::map2_df( c("unif", "K", "notunif"), 
                  function(x,prior) 
                   data.frame(prior, optimal_policy(x, m$observation[,,1], states))
                )-> p
                 

                 
```


```{r}
ggplot(p, aes(state, state - policy, col=prior)) + 
  geom_point(alpha = 0.5)
```


The value of a state $x$ given belief $b(x)$ is given by $V(b) = \max_i \sum_{x \in X} b(x) \alpha_i(x)$

 each $\alpha_i$ is represented as a column in `alpha` data frame, with each row the corresponding to a different
 state $x$.  The number i \in 1...n where n is the number of piecewise linear segments being used to approximate the value function,
 each segmenet associated to an action given by `alpha_action`; that is, the `j`th column of `alpha` corresponds to the action
 given by `alpha_action[j]`.  


Compare to old pomdp solution:

```{r}
system.time(soln <- pomdp(m$transition, m$observation, m$reward, discount, precision = precision))
```

```{r}
old_method <- data.frame(prior = "old method", policy = soln$policy, value = soln$value, state = states)

rbind(p, old_method) %>%
  ggplot(aes(state, state - policy, col=prior)) + 
  geom_point(alpha = 0.5)
```



--------


Compare to non-matrix solution at a single point

```{r}

A <- unif
O <- m$observation[,,1]
alpha <- unname(as.data.frame(A$alpha))
alpha_action <- A$alpha_action
V <- t(alpha) %*% O 

## same as manual
belief <- m$observation[,2,1]
x2 <- vapply(alpha, function(x) belief %*% x, double(1))



identical(V[,2], x2)
value_2 <- max(x2)
policy_2 <- alpha_action[which.max(x2)]
```
      
