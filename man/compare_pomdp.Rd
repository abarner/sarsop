% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compare_pomdp.R
\name{compare_pomdp}
\alias{compare_pomdp}
\title{compare_pomdp}
\usage{
compare_pomdp(transition, observation, reward, discount, obs, action,
  state_prior = rep(1, dim(observation)[[1]])/dim(observation)[[1]],
  alpha = NULL, ...)
}
\arguments{
\item{transition}{Transition matrix, dimension n_s x n_s x n_a}

\item{observation}{Observation matrix, dimension n_s x n_z x n_a}

\item{reward}{reward matrix, dimension n_s x n_a}

\item{discount}{the discount factor}

\item{obs}{a given sequence of observations}

\item{state_prior}{initial belief state, optional, defaults to uniform over states}

\item{alpha}{the matrix of alpha vectors returned by \code{\link{sarsop}}}

\item{...}{additional arguments to \code{\link{appl}}.}

\item{x0}{initial state}

\item{a0}{initial action (default is action 1, e.g. can be arbitrary
if the observation process is independent of the action taken)}

\item{actions}{the corresponding sequence of actions}
}
\value{
a list, containing: a data frame with columns for time, obs, action, and optimal action,
and an array containing the posterior belief distribution at each time t
}
\description{
compare_pomdp
}
\examples{
\dontrun{ ## Takes > 5s
## Use example code to generate matrices for pomdp problem:
source(system.file("examples/fisheries-ex.R", package = "appl"))
alpha <- sarsop(transition, observation, reward, discount, precision = 10)
sim <- sim_pomdp(transition, observation, reward, discount, obs = rnorm(15, 21), action = rep(1, 21),
                     x0 = 5, alpha = alpha)

}

}

