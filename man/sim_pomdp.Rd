% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/sim_pomdp.R
\name{sim_pomdp}
\alias{sim_pomdp}
\title{sim_pomdp}
\usage{
sim_pomdp(transition, observation, reward, discount, policy, x0, Tmax, a0 = 1)
}
\arguments{
\item{transition}{Transition matrix, dimension n_s x n_s x n_a}

\item{observation}{Observation matrix, dimension n_s x n_z x n_a}

\item{reward}{Reward matrix, dimension n_s x n_a}

\item{discount}{the discount factor}

\item{policy}{the policy to be simulated. Should be a vector of length n_s where
the ith element gives the action (index of the action) for an observation of the ith state,
(i.e. as returned by pomdp or appl functions)}

\item{x0}{initial state}

\item{Tmax}{duration of simulation}

\item{a0}{initial action (default is action 1, e.g. can be arbitrary
if the observation process is independent of the action taken)}
}
\value{
a data frame with columns for time, state, obs, action, and (discounted) value.
}
\description{
sim_pomdp
}
\details{
simulation assumes the following order of updating: For system in state[t] at
time t, an observation of the system obs[t] is made, and then action[t] is based on that
observation and the given policy, returning (discounted) reward[t].
}

