% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pomdp_solve.R
\name{run_pomdp}
\alias{run_pomdp}
\title{run_pomdp}
\usage{
run_pomdp(transition, observation, utility, discount, initial = rep(1,
  dim(observation)[[1]])/dim(observation)[[1]], verbose = TRUE, ...)
}
\arguments{
\item{transition}{Transition matrix, dimension n_s x n_s x n_a}

\item{observation}{Observation matrix, dimension n_s x n_z x n_a}

\item{utility}{Utility/reward matrix, dimension n_s x n_a}

\item{discount}{the discount factor}

\item{initial}{initial belief state, optional, defaults to uniform over states}

\item{verbose}{logical, should the function include a message with pomdp diagnostics (timings, final precision, end condition)}

\item{...}{additional arguments to appl SARSOP algorithm, see \code{\link{appl}}.}
}
\value{
a matrix of alpha vectors. Column index indicates action associated with the alpha vector, (1:n_actions),
 rows indicate system state, x. Actions for which no alpha vector was found are included as all -Inf, since such actions are
 not optimal regardless of belief, and thus have no corresponding alpha vectors in alpha_action list.
}
\description{
run_pomdp wraps the tasks of writing the pomdpx file defining the problem, running the pomdsol (SARSOP) algorithm in C++,
and then reading the resulting policy file back into R.  The returned alpha vectors and alpha_action information is then
transformed into a more generic, user-friendly repesentation as a matrix whose columns correspond to actions and rows to states.
This function can thus be used at the heart of most pomdp applications.
}

