% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compute_policy.R
\name{compute_policy}
\alias{compute_policy}
\title{compute_policy}
\usage{
compute_policy(alpha, transition, observation, reward, state_prior, a_0 = 1)
}
\arguments{
\item{transition}{Transition matrix, dimension n_s x n_s x n_a}

\item{observation}{Observation matrix, dimension n_s x n_z x n_a}

\item{a_0}{previous action. Belief in state depends not only on observation, but on prior belief of the state and subsequent action that had been taken.}
}
\value{
a data frame providing the optimal policy (choice of action) and corresponding value of the action for each possible belief state
}
\description{
compute_policy
}
\examples{
\dontrun{ ## Takes > 5s
## Use example code to generate matrices for pomdp problem:
source(system.file("examples/fisheries-ex.R", package = "appl"))
alpha <- sarsop(transition, observation, reward, discount, precision = 10)
compute_policy(alpha, transition, observation, reward)
}

}

